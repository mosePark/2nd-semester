\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} % Required for inserting images
\usepackage{neurips_2023}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{kotex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{RoBaMF : Role-Based Multimodal Fusion model\\
for Online News Classification}
\author{M.S Park, J.H ahn, L.S Yoon}
\date{October 2023}

\begin{document}

\maketitle

\begin{abstract}
  The transmission of information is not limited to text, but also includes various forms such as images, videos and voices. Thus, the use of multimodal models in data analysis is gaining attention. In this paper, we propose a multimodal model called RoBaMF that reflects both text data and image data from online newspapers in the analysis. The RoBaMF includes a feature-fusion model to reflect the interaction between images and annotations. Additionally, we consider an ensemble methodology that also takes into account single models of text and images. According to our research results, RoBaMF demonstrates improved accuracy compared to article-image composite models.
\end{abstract}

\section{Introduction}
Internet news articles consist of a title, main text, images, and annotations for those images. To categorize these articles, it is reasonable to consider a multimodal model that incorporates both text and image information into the analysis. Moreover, since annotations provide detailed descriptions of the images, there is a strong correlation between images and their accompanying annotations. In this paper, we propose the RoBaMF model, which includes the role of annotations in solving the news article category classification problem. This model incorporates base classifiers using feature-fusion to reflect the interaction between images and their associated annotations. In addition, to reflect the unique information from the article text and images - both of which play significant roles in newspapers - we have incorporated individual base classifiers for each element. The results were produced by applying ensemble methods to these three base classifiers.



\section{Related Work}
\subsection{News classification based on text information}
Suh Y, Yu J, Mo J, Song L. (2017) conducted category classification based on Naver article data using Logistic Regression (LR), SVM, Naïve Bayes, and KNN. Yoon (2014) proposed TextCNN, a CNN document classification model that uses word embedding matrices as input values.\cite{suh2017comparison} Jang B, Kim I, Kim JW (2019) performed tweet and news classification using Word2Vec and CNNs and demonstrated that Word2Vec improves performance by learning semantic relationships between words.\cite{jang2019word2vec} Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, Ahad Ali (2022) combined BERT with LSTM to perform fake news classification.\cite{rai2022fake}


\subsection{Feature Fusion}
Feature fusion refers to the process of merging information extracted from (integrating disparate data sources into a unified representation)or(two different data sources into one), transforming heterogeneous features into a single vector. Simple methods of feature fusion include maximization, summation, and concatenation. Nicolas Audebert, Catherine Herold, Kuider Slimani, Cédric Vidal (2019) employed the concatenation method in their image-text multimodal classification problem.\cite{audebert2020multimodal} In their paper, they stated that models using the summation method performed worse than those using only images (pure-image model), inferring that this was due to summation undermining the unique discriminating power of both images and text. As for more complex feature-fusion methods, Jian Yang (2002) proposed parallel feature fusion\cite{yang2003feature} while Yimian Dai (2021) suggested attentional feature fusion.\cite{dai2021attentional}


\subsection{Ensemble methods}
Ensemble methodology is a technique that combines multiple base models to improve the accuracy of predictions. Yawen Xiao, Jun Wu, Zongli Lin, and Xiadong Zhao (2017) presented a deep learning-based multi-model ensemble methodology that demonstrated excellent results.\cite{xiao2018deep} In a study by Silvia Corchs, Elisabetta Fersini \& Francesca (2019), they conducted sentiment analysis applying the BMA (Bayesian Model Averaging) ensemble methodology to image-text multimodal data.\cite{corchs2019ensemble}


\section{Methodology}
\subsection{Textual Features}
In this study, we need to extract features from text data, which includes the title and body text. To do this, we use KoBERT for extracting textual features.
KoBERT, like the original BERT\cite{devlin2018bert}, adopts a Masked Language Model that masks some words and predicts those masked words during training to extract context information in both directions of the input sequence.
KoBERT was developed to overcome some limitations in Korean NLP performance of the existing BERT-base multilingual cased (hereafter MBERT). It was trained on a corpus composed of Korean sentences collected from sources such as Wikipedia and news articles.
To reflect unique changes in the Korean language, it applies the SentencePiece tokenizer that can handle a long sentence as a single token.
KoBERT has shown about 27\% performance improvement in Korean NLP compared to MBERT.
In practice, KoBERT showed superior results than MBERT in sentiment analysis for reviews on a Korean e-commerce platform and for corporate-related Korean news.




\subsection{Visual features}
Using visual features extracted by fine-tuned deep CNNs can be an excellent baseline for visual recognition tasks.\cite{sharif2014cnn} Based on this, we extract deep visual features from news images using fine-tuned MobileNetV2 with ImageNet in this study. MobileNetV2 is a successor model to MobileNet that improves performance while reducing model size and computational load by adopting an inverted residual structure. It consists of stacked bottleneck blocks that sequentially perform the following three stages: narrow pointwise convolution followed by identity activation; 3 by 3 wide depthwise separable convolution followed by ReLU activation; narrow pointwise convolution followed by ReLU activation again. Ke Dong, Chenjie Zhou, Yihan Ruan \& Yuzhi Li (2020) showed better results performing image classification using MobileNet V2 compared to previous models.\cite{dong2020mobilenetv2}



\subsection{Feature fusion}
In this research, image features were first extracted using MobileNetV2, and text features were obtained using KoBERT. These features were then input into a feedforward NN model. "To do so, two feature vectors were combined by stacking them together into a single vector."
This feature fusion method, known as concatenation, has several advantages:\\

1. Information Preservation\\
Concatenation simply connects feature vectors without the need to consider the size or characteristics of the two feature vectors being combined. As a result, it completely retains the features derived from each domain, minimizing information loss. Therefore, it is advantageous for multi-modal data that requires the preservation of the discriminating power of each domain.\\

2. Flexibility\\
Concatenation remains unaffected even if the feature extraction method (feature extractor) is changed. For example, in this paper, it is possible to replace MobileNetV2, which is considered for image feature extraction, with another state-of-the-art deep CNN model like ResNet.\\


\subsection{Ensemble Methods}
This research aims not only to consider interactions between images and annotations but also to consider unique information contained within articles and images.
Additionally, we intend to understand the impact of the interaction between images and annotations on the performance of the model in news category classification problem.
To address both these considerations, weighted soft-voting ensemble and stacking ensemble methodology were applied onto the final model.
Unlike traditional soft-voting that simply sums up prediction probabilities of base classifiers, weighted soft-voting assigns weights based on classification performance of the base models.
Therefore, it allows us to assess the impact of the interaction between images and annotations on the classification performance through these weights.
Stacking ensemble methodology uses prediction probabilities from base classifiers (level 1 classifiers) as input for a meta model (level 2 classifier or meta learner). By utilizing lightGBM, a decision tree-based model as the meta model, it allows the calculation of feature importance. Hence, similar to weighted soft-voting, it is possible to understand the impact of image-annotation interaction models on classification through feature importance.
The multimodal classification model created through these processes is referred to as Role-Based Multimodal Fusion Model (RoBaMF). The overall pipeline of the RoBaMF model is shown in Fig 1.


\section{Experiments}
\subsection{Dataset}
Naver internet news aggregator is divided into six major categories: Politics, Economy, Society, Life/Culture, IT/Science, and World. We crawled the latest 500 news articles from each of these six categories. News without images were excluded from the crawling target. Also, news containing data not suitable for this research were filtered out after crawling. The dataset for this study consists of category, title, body text and image-annotation pairs of 3000 news articles.


\subsection{Models}
This subsection describes the implementation of our RoBaMF model along with some comparative models. All models are implemented in TensorFlow.



\textbf{Article Text Baseline}\quad
The KoBERT model is used to extract information from title and body text.
The input dataset consists of a list of ['title + body', 'category'] for each article.
Since we perform classification for six categories in this study, a vector of length 6 is appended to the output layer.
From now on, this model will be referred to as "Article".


\textbf{Image Baseline}\quad
The MobileNetV2 model is used to extract information from photos and diagrams.
Pre-trained weights on ImageNet are loaded which enhances the discriminating power of image features through such transfer learning.
Also, image sizes are adjusted to (224, 224, 3) so that they fit the input shape required by MobileNetV2 running on Keras.
From now on this model will be referred to as "Image".


\textbf{Feature Fusion Baseline}\quad
This is a model designed to ascertain the discriminative power of image-text multimodal features. Text and images are separately input into KoBERT and MobileNetV2, and features from both domains are extracted by setting the output layer length to 128. Subsequently, the concatenation method is applied to combine the features from both domains into a single vector with a length of 256, followed by connecting it to a dense layer with a length of 6.
The model using article text data will be referred as "Article-Fusion" and one using annotation text will be called "Annotation-Fusion".


\textbf{Ensemble Competitor Models}\quad
These are competitor models designed to compare the performance of our proposed RoBaMF model. Using Article, Image, and Article-Fusion models as base classifiers, ensemble methodologies are applied. For ensemble techniques, we refer to the model that uses weighted soft-voting as 'Voting-Competitor' and the model that uses stacking as 'Stacking-Competitor'.\\


\textbf{RoBaMF Model}\quad
These are models that consider the unique features of articles and images, as well as the interaction between images and annotations. They use Article, Image, and Annotation-Fusion models as base classifiers and apply ensemble methodologies.
For ensemble techniques, we refer to the model that uses weighted soft-voting as 'Voting-RoBaMF' and the model that uses stacking as 'Stacking-RoBaMF'."

\\

\textbf{K-fold Cross Validation}\quad
Each model's performance is evaluated through K-fold cross-validation. Except for Stacking-Competitor and Stacking-RoBaMF models, a standard K-fold methodology was applied to other models. For each fold, the data was divided into 2400 training samples and 600 test samples. The accuracy was averaged over 5 runs.
K-fold cross validation designed for stacking ensembles is applied to the Stacking-Competitor and Stacking-RoBaMF models.
Data gets split into five folds like standard K-fold methodology.
Base classifiers get trained on K-1 subsets. Then trained classifiers are applied to the remaining one subset and the output of all the base classifiers constitute the input feature space of the meta learner.
Detailed operation can be found in "Data Classification: Algorithms and Applications".

\bibliography{ref}
\bibliographystyle{plain}



\end{document}
