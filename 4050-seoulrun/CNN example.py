# -*- coding: utf-8 -*-
"""MNIST

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10a09cZzYRxLldenfc3RcBFFoyXZ2rz--
"""
# 출처 : 서울시립대학교 전종준교수님 특강 (4050 서울런)
  
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

train_dataset = torchvision.datasets.MNIST(root='./data',
                                           train=True,
                                           transform=transforms.ToTensor(),
                                           download=True)

test_dataset = torchvision.datasets.MNIST(root='./data',
                                          train=False,
                                          transform=transforms.ToTensor())

i = 100
image = train_dataset.data[i].float()/255
label = train_dataset.targets[i]

plt.imshow(image, cmap='gray')
plt.title('Label: %d' % label)
plt.show()

images = [train_dataset[i][0] for i in range(100)]
# 타일 형식으로 이미지 시각화
grid_img = torchvision.utils.make_grid(images, nrow=10, ncols = 10)
plt.imshow(grid_img.permute(1, 2, 0))
plt.show()

"""
torchvision.utils.make_grid 함수가 흑백 이미지의 채널을 변화시켜서, (채널, 세로, 가로)이미지를 생성한다. 하지만 imshow 함수는 채널을 먼저 받을 수 없어서 (세로, 가로, 채널)로 데이터를 변화한다. permute 함수를 사용하여 텐서의 차원을 재배열함"""

print("dataset:",train_dataset[i][0].shape)
grid_img = torchvision.utils.make_grid(images, nrow=10, ncols = 10)
print("grid.img:", grid_img.shape)
print("permute image", grid_img.permute(1, 2, 0).shape)

batchsize = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batchsize)

x = train_dataset.data[0].float()/255
print(x.shape)
x = x.reshape(1,28,28)

layer1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
                      nn.ReLU(),
                      nn.MaxPool2d(kernel_size=2, stride=2))

h1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)(x)
h1.shape

h1 = nn.ReLU()(h1)
h1.shape

h1 = nn.MaxPool2d(kernel_size = 2, stride = 2)(h1)
h1.shape

layer1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
                      nn.ReLU(),
                      nn.MaxPool2d(kernel_size=2, stride=2))
h1 = layer1(x)
h1.shape

layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
                      nn.ReLU(),
                      nn.MaxPool2d(kernel_size=2, stride=2))
h2 = layer2(h1)
h2.shape

layer3 = nn.Sequential(nn.Flatten(0,-1),
                       nn.Linear(64*7*7,10),
                       nn.Softmax(dim=0))
output = layer3(h2)
output.shape

class MyNeuralNetwork(nn.Module):
    def __init__(self):
        super(MyNeuralNetwork, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride =1, padding= 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride =1, padding= 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.layer3 = nn.Sequential(
            nn.Linear(in_features=64*7*7, out_features=128, bias=True),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64, bias=True),
            nn.Sigmoid(),
            nn.Linear(in_features=64, out_features=10, bias=True),
            nn.Softmax(dim = 1)
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = x.view(x.shape[0], -1)
        x = self.layer3(x)
        return x

model = MyNeuralNetwork()

for i, (data, target) in enumerate(train_loader):
  output = model(data)
  if i==0:
    break
output[0]

device = 'cuda' if torch.cuda.is_available() else 'cpu'

torch.manual_seed(1)
if device == 'cuda':
    torch.cuda.manual_seed_all(1)

device

model = MyNeuralNetwork().to(device)

learning_rate = 0.01
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
cross_entropy = nn.CrossEntropyLoss().to(device)

num_B = 10
loss_vec = np.zeros(num_B)
optimizer.zero_grad()

for i in range(num_B):
  val_vec = []
  if i == 5:
    print("Learning rate is updated")
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate/2)
  for j, (data, target) in enumerate(train_loader):
    data = data.to(device)
    target = target.to(device)
    output = model(data)
    loss = cross_entropy(output, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    predicted = torch.max(output.data,1)
    v =((predicted.indices == target).sum()).item()/len(target)
    val_vec.append(v)
  print ("Accuracy: {:.4f}".format(np.array(val_vec).mean()))

test_loader = torch.utils.data.DataLoader(test_dataset, batchsize)
for i, (dataset, target) in enumerate(test_loader):
  print(dataset[0,:,:,:].shape)
  break

v = 0
for i, (dataset, target) in enumerate(test_loader):
  X = dataset.to(device)
  Y = target.to(device)
  output = model(X)
  predicted = torch.max(output.data,1)
  v = v + ((predicted.indices == Y).sum()).item()
print(v)

"Accuracy is {:.4f}%".format(v/len(test_dataset.targets))
