\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{kotex}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.8pt} \\
		\LARGE \textbf{\uppercase{비지도학습}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{수업 리뷰} \vspace*{10\baselineskip}}
		}
\date{}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------
\section{소개}
지도학습과 비지도학습의 차이는 반응변수의 유무에 있다. \textit{지도학습}이란, 입력 또는 예측변수의 값들이 주어지고 반응변수가 출력되는 개념이다. 지도학습이 관심이 있는 것은 결국 "$Pr(Y \vert X)$" 이다. 여기서 반응변수는 $Y=(Y_1, ..., Y_m)$, 입력 또는 예측변수는 $X^T=(X_1, ..., X_p)$ 그리고 $x_i^T = (x_i_1, ..., x_i_p)$는 i번째 훈련된 케이스, $y_i$는 반응변수의 측정이다.
\\ \\
반면에 \textit{비지도학습}은 반응변수가 없다. 그렇기 때문에 평가(evaluation) 방식이 추상적이다. 비지도학습의 경우 관심있는 것은 "$Pr(X)$" 이다. 이 경우 랜덤 p-벡터 $X$의 $N$개 관측값 집합인 $(x_1, x_2, ..., x_N)$이 주어지고, 이 확률변수 $X$의 확률 밀도를 추정하는 것이 목표이다.
\\ \\
p의 크기가 작을 때는 확률밀도추정에 다양한 비모수적인 방법이 있지만, 그 크기가 클 때는 문제가 복잡할뿐더러 차원의 저주로 인해 문제해결이 어렵다. 즉, pdf를 추정하는 것이 어렵고 정보량이 부족하기 때문에 이론상 불가능하다.
\\ \\
그러므로 일반적으로는 이러한 기술 통계 방법론에서 $Pr(X)$가 비교적 큰 값인 X 값이나 해당 값들의 집합을 특성화하려고 한다. 클러스터링의 경우 2차원 실수평면상에서 multimodal 형태의 부분에서 각 peak 부분에 해당하는 곳을 X축 상에서 정확히 점찍는 것이 pdf를 알아내는 방법이 되겠다. % 데이터의 밀도가 가장 큰 부분을 찾는 문제 (교수님 설명, 시각화 예시 추가)

\section{연관성 규칙}

연관성 규칙 분석은 상업용 데이터베이스에서 마이닝할 수 있는 유명한 방법이다. 이 방법론의 목표는 해당 데이터베이스에서 빈번히 나타나는 변수 $X = (X_1, ..., X_p)$의 joint values를 찾는 것이다. 보통 각 값들이 이진 형태 $X_j \in (0, 1)$ 이다.
\\ \\
연관성 규칙의 기본 목표는 특성벡터 X에 대한 프로토타입 X-값$v_1, ..., v_L$들의 Collection을 찾아내는 것이다. 각각의 값들에서 평가된 확률밀도 $Pr(v_l)$가 상대적으로 크다. 다시말해 카테고리가 많아지면 경우의수가 그만큼 커지기 때문에 "mode 찾기"와 같은 문제가 어려워진다는 것을 말한다. 따라서 문제를 심플하게 접근하는 것이 필요하다.
\\ \\
$Pr(X)$가 큰 곳에서 $x$값을 찾는 것이 아닌 크기나 서포트와 비교해 상대적으로 큰 확률의 x값을 가진 X-공간상에서 구간을 찾는 것이 더 쉽다.
\\ \\
$\mathcal{S}_j$가 j번째 변수의 모든 가능한 값들의 집합을 나타내고, $s_j$ \subset $\mathcal{S}_j$ 라 하자.

\begin{align}
    Pr\left [ \bigcap_{j=1}^{p}(X_j\in s_j) \right ],
\end{align}

\\ \\
$Pr()$안에 있는 식은 \textbf{conjunctive rule}이라고 한다. 하지만 식 (1)은 informative 하지 않다.

\subsection{장바구니 분석}
$p \approx 10^4, N \approx 10^8 $인 상용 데이터베이스에서는 달성이 불가능하다. 따라서 식 (1)에서 추가적인 단순화가 요구된다. 

\begin{align}
    Pr\left [ \bigcap_{j \in \mathcal{J}}(X_j = v_0_j) \right ],
\end{align}

\\ \\
식 (2)의 경우, $s_j$가 특정한 단일 값이나 전체 집합 $s_j = \mathcal{S}_j$ 으로 구성된다. 위 식 (2)가 크도록 $v_0$, $j \in \mathcal{J}$ 값을 찾는 것이 목표다.
단, $\mathcal{J} \subset \left\{1, ..., p\right\}$ 이다.







\end{document}
